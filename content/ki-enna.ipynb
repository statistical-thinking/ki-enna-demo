{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a005a762-a9c7-43c5-9e96-c1520595328d",
   "metadata": {},
   "source": [
    "# KI-ENNA: (E)in (N)euronales (N)etz zum (A)usprobieren\n",
    "Live-Demo basierend auf KI-ENNA 2.0.1 von Prof. Dr. habil. Dennis Klinkhammer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c083f5f-f8f7-413d-b0f4-d06fc726f49c",
   "metadata": {},
   "source": [
    "![title](https://github.com/statistical-thinking/ki-enna-demo/blob/main/content/img/image1.png?raw=true)\n",
    "## Aufgabenstellung\n",
    "Die obere Darstellung zeigt drei **verschiedene Pflanzenarten**, deren **Blattlänge und Blattbreite** gemessen worden sind. Während die mit Kreisen dargestellte Planzenart sich eindeutig von den beiden anderen Pflanzenarten unterscheidet, weisen von den anderen Pflanzenarten **einzelne Pflanzen die gleiche Blattlänge und Blattbreite** auf. Daher sind diese Pflanzenarten **schwer zu unterscheiden**. Ein neuronales Netzwerk kann bei der Unterscheidung der ähnlichen Pflanzenarten hilfreich sein."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9358d05a-3cad-42a7-8617-1ffc99df1a7f",
   "metadata": {},
   "source": [
    "![title](https://github.com/statistical-thinking/ki-enna-demo/blob/main/content/img/image2.png?raw=true)\n",
    "## Training eines neuronalen Netzwerks\n",
    "Die interaktive Benutzeroberfläche ermöglicht es Dir, in nur wenigen Schritten Dein eigenes neuronales Netzwerk als **Grundlage einer echten Künstlichen Intelligenz zu erstellen und zu trainieren**. Dabei kannst Du die Anzahl an Schichten, Neuronen und Epochen, aber auch die Lernrate selbst einstellen. Neuronale Netzwerke sind wie kleine Denkmaschinen, **die in Schichten arbeitet**. In die **Eingabeschicht** können Zahlen eingegeben werden, die in den **versteckten Schichten** weiter verarbeitet werden. Am Ende gibt dann die **Ausgabeschicht** eine Antwort. **Neuronen treiben die Denkmaschine an**. Sie rechnen mit den Zahlen und **schicken Zwischenergebnisse an Neuronen in anderen Schichten weiter**. Aber Achtung: Nicht immer führen mehr Schichten und Neuronen zu einem besseren Ergebnis. Damit das funktioniert, braucht ein neuronales Netzwerk Wiederholungen: Eine **Epoche ist ein Durchgang durch alle Übungsaufgaben**. Das macht das neuronale Netzwerk mehrmals, um immer besser zu werden. Die **Lernrate bestimmt, wie schnell das neuronale Netzwerk dabei seine Fehler verbessert**: Ist die Lernrate zu hoch, lernt es zu schnell und macht vielleicht neue Fehler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90cd7138-8ae0-4dff-9675-306e0e8e6333",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# --- Erster Code ---\n",
    "import micropip\n",
    "await micropip.install('ipywidgets')\n",
    "import random, math\n",
    "import matplotlib.pyplot as plt\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, clear_output\n",
    "\n",
    "# --- Dataset ---\n",
    "Xtest = [[ 0.81575475, -0.21746808, -0.12904165, -0.65303909],\n",
    "         [ 0.05761837,  1.59476592,  0.84485761,  1.71304456],\n",
    "         [ 0.96738203,  0.68864892, -0.00730424, -0.41643072],\n",
    "         [ 2.02877297,  0.38660992,  2.06223168,  1.00321947],\n",
    "         [ 1.42226386,  0.99068792,  1.33180724,  0.29339437],\n",
    "         [ 0.81575475,  0.99068792,  1.21006983,  1.4764362 ],\n",
    "         [-1.00377258,  0.38660992, -0.49425387, -0.41643072],\n",
    "         [ 0.05761837, -0.51950708, -0.00730424,  0.29339437],\n",
    "         [ 0.36087292,  0.38660992,  1.08833242,  1.23982783],\n",
    "         [ 0.66412748,  0.38660992,  0.35790798,  1.4764362 ],\n",
    "         [ 0.05761837,  0.08457092,  0.84485761,  0.29339437],\n",
    "         [-0.70051802, -0.51950708,  0.23617057,  0.53000274],\n",
    "         [ 0.20924564, -0.21746808,  0.84485761,  1.00321947],\n",
    "         [-0.24563619,  0.08457092, -0.25077906, -0.65303909],\n",
    "         [-2.06516352, -1.42562408, -1.95510276, -1.59947255],\n",
    "         [-1.15539985, -1.42562408, -1.34641572, -1.36286418],\n",
    "         [ 0.05761837, -1.12358508, -0.00730424, -0.41643072],\n",
    "         [ 0.20924564,  0.08457092, -0.73772869, -0.88964745],\n",
    "         [-0.39726347, -0.51950708,  0.23617057, -0.17982236],\n",
    "         [ 0.5125002 ,  0.08457092, -0.37251647, -0.88964745]]\n",
    "\n",
    "ytrue = [0,1,0,1,1,1,0,1,1,1,1,1,1,0,0,0,0,0,0,0]\n",
    "\n",
    "# --- Activation & Loss ---\n",
    "def sigmoid(x): return 1 / (1 + math.exp(-x))\n",
    "def sigmoid_derivative(out): return out * (1 - out)\n",
    "def relu(x): return max(0, x)\n",
    "def relu_derivative(out): return 1 if out > 0 else 0\n",
    "def binary_cross_entropy(pred, y): \n",
    "    epsilon = 1e-7\n",
    "    return - (y * math.log(pred + epsilon) + (1 - y) * math.log(1 - pred + epsilon))\n",
    "\n",
    "def binary_cross_entropy_derivative(pred, y): \n",
    "    epsilon = 1e-7\n",
    "    return -(y / (pred + epsilon)) + (1 - y) / (1 - pred + epsilon)\n",
    "\n",
    "# --- Dense Layer ---\n",
    "def dense_forward(x, w, b, act='relu'):\n",
    "    outs, pres = [], []\n",
    "    for j in range(len(w)):\n",
    "        z = sum(x[i] * w[j][i] for i in range(len(x))) + b[j]\n",
    "        pres.append(z)\n",
    "        outs.append(sigmoid(z) if act == 'sigmoid' else relu(z))\n",
    "    return outs, pres\n",
    "\n",
    "def dense_backward(x, grad_out, out, pre, w, b, act='relu', lr=0.01):\n",
    "    grad_in = [0] * len(x)\n",
    "    for j in range(len(w)):\n",
    "        delta = grad_out[j] * (sigmoid_derivative(out[j]) if act == 'sigmoid' else relu_derivative(pre[j]))\n",
    "        for i in range(len(x)):\n",
    "            grad_in[i] += w[j][i] * delta\n",
    "            w[j][i] -= lr * delta * x[i]\n",
    "        b[j] -= lr * delta\n",
    "    return grad_in\n",
    "\n",
    "# --- Visualisierung ---\n",
    "def plot_network_architecture(layer_sizes, input_dim):\n",
    "    all_layers = [input_dim] + layer_sizes + [1]\n",
    "    fig, ax = plt.subplots(figsize=(max(8, len(all_layers) * 1.5), 5))\n",
    "    ax.axis('off')\n",
    "    v_spacing = 1.5\n",
    "    h_spacing = 2\n",
    "\n",
    "    for i, n_neurons in enumerate(all_layers):\n",
    "        layer_x = i * h_spacing\n",
    "        layer_y_start = -(n_neurons - 1) * v_spacing / 2\n",
    "        for j in range(n_neurons):\n",
    "            circle = plt.Circle((layer_x, layer_y_start + j * v_spacing), 0.3, color='black', ec='black')\n",
    "            ax.add_patch(circle)\n",
    "            if i == 0:\n",
    "                ax.text(layer_x - 0.6, layer_y_start + j * v_spacing, f'x{j+1}', ha='center', va='center')\n",
    "            elif i == len(all_layers)-1:\n",
    "                ax.text(layer_x + 0.6, layer_y_start + j * v_spacing, f'ŷ', ha='center', va='center')\n",
    "        if i > 0:\n",
    "            prev_neurons = all_layers[i-1]\n",
    "            for pj in range(prev_neurons):\n",
    "                for cj in range(n_neurons):\n",
    "                    x1 = (i-1) * h_spacing\n",
    "                    y1 = -(prev_neurons - 1) * v_spacing / 2 + pj * v_spacing\n",
    "                    x2 = i * h_spacing\n",
    "                    y2 = -(n_neurons - 1) * v_spacing / 2 + cj * v_spacing\n",
    "                    ax.plot([x1, x2], [y1, y2], color='gray', lw=0.5)\n",
    "    ax.set_xlim(-1, len(all_layers) * h_spacing)\n",
    "    ax.set_ylim(-6, 6)\n",
    "    plt.show()\n",
    "\n",
    "# --- Speicher für Evaluation ---\n",
    "trained_model = {}\n",
    "\n",
    "# --- Training ---\n",
    "def train_model(X, y, layer_sizes, epochs, lr):\n",
    "    dims = [len(X[0])] + layer_sizes + [1]\n",
    "    weights, biases = [], []\n",
    "    for i in range(len(dims)-1):\n",
    "        w = [[random.uniform(-0.5, 0.5) for _ in range(dims[i])] for _ in range(dims[i+1])]\n",
    "        b = [random.uniform(-0.5, 0.5) for _ in range(dims[i+1])]\n",
    "        weights.append(w)\n",
    "        biases.append(b)\n",
    "\n",
    "    loss_trace = []\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        for xi, yi in zip(X, y):\n",
    "            x = xi\n",
    "            acts, pres = [], []\n",
    "            for i in range(len(weights)):\n",
    "                act = 'sigmoid' if i == len(weights)-1 else 'relu'\n",
    "                x, pre = dense_forward(x, weights[i], biases[i], act)\n",
    "                acts.append(x)\n",
    "                pres.append(pre)\n",
    "            loss = binary_cross_entropy(acts[-1][0], yi)\n",
    "            total_loss += loss\n",
    "            grad = [binary_cross_entropy_derivative(acts[-1][0], yi)]\n",
    "            for i in reversed(range(len(weights))):\n",
    "                act = 'sigmoid' if i == len(weights)-1 else 'relu'\n",
    "                inp = xi if i == 0 else acts[i-1]\n",
    "                grad = dense_backward(inp, grad, acts[i], pres[i], weights[i], biases[i], act, lr)\n",
    "        loss_trace.append(total_loss)\n",
    "\n",
    "    return {\n",
    "        \"weights\": weights,\n",
    "        \"biases\": biases,\n",
    "        \"loss_trace\": loss_trace\n",
    "    }\n",
    "\n",
    "# --- Widgets ---\n",
    "layers_slider = widgets.IntSlider(value=2, min=1, max=5, step=1, description='Layeranzahl:')\n",
    "neuron_sliders_box = widgets.VBox()\n",
    "epochs_slider = widgets.IntSlider(value=100, min=10, max=500, step=10, description='Epochen:')\n",
    "lr_slider = widgets.FloatSlider(value=0.05, min=0.001, max=1.0, step=0.01, description='Lernrate:')\n",
    "train_button = widgets.Button(description=\"Trainieren\")\n",
    "train_output = widgets.Output()\n",
    "\n",
    "# --- Neuronenslider aktualisieren ---\n",
    "def update_neuron_sliders(*args):\n",
    "    count = layers_slider.value\n",
    "    sliders = []\n",
    "    for i in range(count):\n",
    "        sliders.append(widgets.IntSlider(value=4, min=1, max=10, step=1, description=f'Neuronen {i+1}:'))\n",
    "    neuron_sliders_box.children = sliders\n",
    "\n",
    "layers_slider.observe(update_neuron_sliders, names='value')\n",
    "update_neuron_sliders()\n",
    "\n",
    "# --- Callback für Training & Visualisierung ---\n",
    "def on_train_click(b):\n",
    "    train_output.clear_output()\n",
    "    layer_sizes = [s.value for s in neuron_sliders_box.children]\n",
    "    epochs = epochs_slider.value\n",
    "    lr = lr_slider.value\n",
    "\n",
    "    model = train_model(Xtest, ytrue, layer_sizes, epochs, lr)\n",
    "    trained_model.clear()\n",
    "    trained_model.update({\n",
    "        \"weights\": model[\"weights\"],\n",
    "        \"biases\": model[\"biases\"],\n",
    "        \"loss_trace\": model[\"loss_trace\"],\n",
    "        \"layer_sizes\": layer_sizes\n",
    "    })\n",
    "\n",
    "    with train_output:\n",
    "#       print(\"Trainiertes Netzwerk:\")\n",
    "        plot_network_architecture(layer_sizes, input_dim=len(Xtest[0]))\n",
    "\n",
    "train_button.on_click(on_train_click)\n",
    "\n",
    "# --- Anzeige ---\n",
    "display(widgets.VBox([\n",
    "    layers_slider,\n",
    "    neuron_sliders_box,\n",
    "    epochs_slider,\n",
    "    lr_slider,\n",
    "    train_button,\n",
    "    train_output\n",
    "]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15a39894-fbf6-4a4c-9e4d-2f719efc2ffe",
   "metadata": {},
   "source": [
    "![title](https://github.com/statistical-thinking/ki-enna-demo/blob/main/content/img/image3.png?raw=true)\n",
    "## Bewertung eines neuronalen Netzwerks\n",
    "Die **Verlust-Funktion ist wie ein Schiedsrichter**. Sie schaut sich die Antworten des neuronalen Netzwerks an und entscheidet, ob das Ergebnis richtig oder falsch ist. Wenn ein neuronales Netzwerk falsch liegt, zeigt die Verlust-Funktion **wie weit die Antwort von der richtigen Lösung entfernt ist**. So lernt das neuronale Netzwerk, was es besser machen muss. Die Antworten eines neuronalen Netzwerks können in einer **Confusion Matrix** zusammengefasst werde. Das ist eine **Tabelle, die zeigt, wie oft ein neuronales Netzwerk etwas richtig oder falsch erkannt hat**. Daraus kann man dann die sogenannte **Accuracy** berechnen, also **wie genau das neuronale Netzwerk insgesamt war**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cb7edc2-c13d-4fcc-a286-3e253a9e68d9",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# --- Zweiter Code ---\n",
    "from IPython.display import display\n",
    "eval_button = widgets.Button(description=\"Bewerten\")\n",
    "eval_output = widgets.Output()\n",
    "\n",
    "def predict(x, weights, biases):\n",
    "    for i in range(len(weights)):\n",
    "        act = 'sigmoid' if i == len(weights)-1 else 'relu'\n",
    "        x, _ = dense_forward(x, weights[i], biases[i], act)\n",
    "    return 1 if x[0] > 0.5 else 0\n",
    "\n",
    "def on_eval_click(b):\n",
    "    eval_output.clear_output()\n",
    "    if not trained_model:\n",
    "        with eval_output:\n",
    "            print(\"Bitte zuerst das Modell trainieren.\")\n",
    "        return\n",
    "\n",
    "    weights = trained_model[\"weights\"]\n",
    "    biases = trained_model[\"biases\"]\n",
    "    losses = trained_model[\"loss_trace\"]\n",
    "\n",
    "    ypred = [predict(xi, weights, biases) for xi in Xtest]\n",
    "    TP = TN = FP = FN = 0\n",
    "    for true, pred in zip(ytrue, ypred):\n",
    "        if true == pred:\n",
    "            if true == 1: TP += 1\n",
    "            else: TN += 1\n",
    "        else:\n",
    "            if true == 1: FN += 1\n",
    "            else: FP += 1\n",
    "    acc = (TP + TN) / len(ytrue)\n",
    "\n",
    "    with eval_output:\n",
    "        plt.plot(losses)\n",
    "        plt.title(\"Verlust-Funktion\")\n",
    "        plt.xlabel(\"Epoche\")\n",
    "        plt.ylabel(\"Verlust\")\n",
    "        plt.grid(True)\n",
    "        plt.show()\n",
    "\n",
    "        print(\"Confusion Matrix:\")\n",
    "        print(f\"TN: {TN}  FP: {FP}\")\n",
    "        print(f\"FN: {FN}  TP: {TP}\")\n",
    "        print(f\"Genauigkeit: {acc:.2f}\")\n",
    "\n",
    "eval_button.on_click(on_eval_click)\n",
    "display(widgets.VBox([eval_button, eval_output]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dadf91b1-8928-422b-a25d-f7385cc3e6ab",
   "metadata": {},
   "source": [
    "**Prima**, jetzt hast Du ein eigenes neuronales Netzwerk **trainiert und bewertet**!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "230f72dd-8506-472d-934e-c4a7341c38a0",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  },
  "toc": {
   "base_numbering": 0
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
