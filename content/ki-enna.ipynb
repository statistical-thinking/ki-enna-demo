{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a005a762-a9c7-43c5-9e96-c1520595328d",
   "metadata": {},
   "source": [
    "![Logo](https://github.com/statistical-thinking/ki-enna-demo/blob/main/content/img/ki-enna-logo.png?raw=true)\n",
    "## ‚öôÔ∏è - Anleitung\n",
    "Mit **KI-ENNA** ([Homepage](https://www.statistical-thinking.de/ki-enna.html)), (E)inem (N)euronalen (N)etz zum (A)usprobieren, kann in wenigen Schritten ein eigenes **neuronales Netzwerk** ([Wiki](https://en.wikipedia.org/wiki/Neural_network_(machine_learning))) aus dem Kontext des **Deep Learnings** ([Wiki](https://en.wikipedia.org/wiki/Deep_learning)) und als methodische Grundlage der **K√ºnstlichen Intelligenz** ([Wiki](https://en.wikipedia.org/wiki/Artificial_intelligence)) generiert, trainiert und evaluiert werden. Die Bedienung erfolgt √ºber ein interaktives **Jupyter Notebook** ([Wiki](https://en.wikipedia.org/wiki/Project_Jupyter#Jupyter_Notebook)), bei dem die entsprechenden Codes in **Python** ([Wiki](https://en.wikipedia.org/wiki/Python_(programming_language))) nacheinander ausgef√ºhrt werden k√∂nnen. Die Codes k√∂nnen √ºber *View / Expand* eingeblendet und *View / Collapse* ausgeblendet sowie √ºber *Run* ausgef√ºhrt werden.\n",
    "\n",
    "## üì§ - Datensatz\n",
    "Im ersten Schritt kann ein **CSV** ([Wiki](https://en.wikipedia.org/wiki/Comma-separated_values)) **Datensatz** ([Wiki](https://en.wikipedia.org/wiki/Data_set)) hochgeladen werden, bei dem die Zielvariable der **Klassifikation** ([Wiki](https://en.wikipedia.org/wiki/Classification)) in der ersten **Spalte** ([Wiki](https://en.wikipedia.org/wiki/Column_(database))) hinterlegt sein sollte und die dazugeh√∂rigen **Features** ([Wiki](https://en.wikipedia.org/wiki/Feature_(machine_learning))) entsprechend in den nachfolgenden Spalten. Der **IRIS** [Wiki](https://en.wikipedia.org/wiki/Iris_flower_data_set)) Datensatz im Ordner *data* ist ein geeignetes Klassifikationsbeispiel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c2395cd-6440-4ebd-8a4a-1365c5a66358",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# --- Erster Teil des Codes ---\n",
    "import micropip\n",
    "await micropip.install('ipywidgets')\n",
    "import random, math\n",
    "from IPython.display import display, clear_output, HTML\n",
    "from ipywidgets import FileUpload\n",
    "import ipywidgets as widgets\n",
    "\n",
    "# Upload widget\n",
    "uploader = FileUpload(accept='.csv', multiple=False)\n",
    "display(uploader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d9c8242-c670-4dee-a4fc-5981e682f0dc",
   "metadata": {},
   "source": [
    "## üß† - Neuronales Netzwerk\n",
    "Danach k√∂nnen die Anzahl an **Schichten** ([Wiki](https://en.wikipedia.org/wiki/Hidden_layer)) und **Neuronen** ([Wiki](https://en.wikipedia.org/wiki/Artificial_neuron)) - sowie die zugrundeliegenden **Aktivierungsfunktionen** ([Wiki](https://en.wikipedia.org/wiki/Activation_function)) - gemeinsam mit den **Epochen** ([Wiki](https://en.wikipedia.org/wiki/Learning_curve_(machine_learning))) und der **Lernrate** ([Wiki](https://en.wikipedia.org/wiki/Learning_rate)) des neuronalen Netzwerks spezifiziert werden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90cd7138-8ae0-4dff-9675-306e0e8e6333",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# --- (2) Hidden Code ---\n",
    "def parse_csv_string(csv_string):\n",
    "    lines = csv_string.strip().split('\\n')\n",
    "    y = []\n",
    "    X = []\n",
    "    for line in lines:\n",
    "        parts = line.strip().split(',')\n",
    "        if len(parts) < 2:\n",
    "            continue\n",
    "        y.append(int(parts[0]))\n",
    "        x_row = [float(val) for val in parts[1:]]\n",
    "        X.append(x_row)\n",
    "    return X, y\n",
    "\n",
    "# --- Split CSV ---\n",
    "if uploader.value:\n",
    "    uploaded_file = uploader.value[0]  # Corrected from .values()\n",
    "    content = uploaded_file['content'].tobytes().decode('utf-8')\n",
    "    X, y = parse_csv_string(content)\n",
    "\n",
    "# --- Standardization ---\n",
    "def normalize(X):\n",
    "    transposed = list(zip(*X))\n",
    "    mins = [min(col) for col in transposed]\n",
    "    maxs = [max(col) for col in transposed]\n",
    "    return [[(x_i - min_i) / (max_i - min_i + 1e-9) \n",
    "             for x_i, min_i, max_i in zip(x_row, mins, maxs)] for x_row in X]\n",
    "\n",
    "X = normalize(X)\n",
    "\n",
    "# --- Activation & Loss ---\n",
    "def sigmoid(x): return 1 / (1 + math.exp(-x))\n",
    "def sigmoid_derivative(out): return out * (1 - out)\n",
    "def relu(x): return max(0, x)\n",
    "def relu_derivative(out): return 1 if out > 0 else 0\n",
    "def leaky_relu(x): return x if x > 0 else 0.01 * x\n",
    "def leaky_relu_derivative(out): return 1 if out > 0 else 0.01\n",
    "def tanh(x): return math.tanh(x)\n",
    "def tanh_derivative(out): return 1 - out**2\n",
    "def binary_cross_entropy(pred, y): \n",
    "    epsilon = 1e-7\n",
    "    return - (y * math.log(pred + epsilon) + (1 - y) * math.log(1 - pred + epsilon))\n",
    "def binary_cross_entropy_derivative(pred, y): \n",
    "    epsilon = 1e-7\n",
    "    return -(y / (pred + epsilon)) + (1 - y) / (1 - pred + epsilon)\n",
    "\n",
    "# --- Dense Layer ---\n",
    "def dense_forward(x, w, b, act='relu'):\n",
    "    pres = [sum(x[i] * w[j][i] for i in range(len(x))) + b[j] for j in range(len(w))]\n",
    "    outs = []\n",
    "    for z in pres:\n",
    "        if act == 'sigmoid':\n",
    "            outs.append(sigmoid(z))\n",
    "        elif act == 'relu':\n",
    "            outs.append(relu(z))\n",
    "        elif act == 'leaky_relu':\n",
    "            outs.append(leaky_relu(z))\n",
    "        elif act == 'tanh':\n",
    "            outs.append(tanh(z))\n",
    "    return outs, pres\n",
    "\n",
    "def dense_backward(x, grad_out, out, pre, w, b, act='relu', lr=0.01):\n",
    "    grad_in = [0] * len(x)\n",
    "    for j in range(len(w)):\n",
    "        if act == 'sigmoid':\n",
    "            delta = grad_out[j] * sigmoid_derivative(out[j])\n",
    "        elif act == 'relu':\n",
    "            delta = grad_out[j] * relu_derivative(pre[j])\n",
    "        elif act == 'leaky_relu':\n",
    "            delta = grad_out[j] * leaky_relu_derivative(pre[j])\n",
    "        elif act == 'tanh':\n",
    "            delta = grad_out[j] * tanh_derivative(out[j])\n",
    "        for i in range(len(x)):\n",
    "            grad_in[i] += w[j][i] * delta\n",
    "            w[j][i] -= lr * delta * x[i]\n",
    "        b[j] -= lr * delta\n",
    "    return grad_in\n",
    "\n",
    "# --- Visualisierung ---\n",
    "def html_network_with_connections(layer_sizes, input_dim):\n",
    "    all_layers = [input_dim] + layer_sizes + [1]\n",
    "    neuron_size = 20\n",
    "    margin = 10\n",
    "    column_spacing = 80\n",
    "    row_spacing = neuron_size + 20\n",
    "    radius = neuron_size // 2\n",
    "    total_width = len(all_layers) * column_spacing\n",
    "    total_height = max(all_layers) * row_spacing + 2 * margin\n",
    "    svg = f'<svg width=\"{total_width}\" height=\"{total_height}\" style=\"position:absolute; top:0; left:0;\">'\n",
    "    positions = []\n",
    "    for li, n in enumerate(all_layers):\n",
    "        layer_x = li * column_spacing + column_spacing // 2\n",
    "        layer = []\n",
    "        total_layer_height = (n - 1) * row_spacing\n",
    "        offset_y = (total_height - total_layer_height) // 2\n",
    "        for ni in range(n):\n",
    "            layer_y = offset_y + ni * row_spacing\n",
    "            layer.append((layer_x, layer_y))\n",
    "        positions.append(layer)\n",
    "    for i in range(len(positions)-1):\n",
    "        for x1, y1 in positions[i]:\n",
    "            for x2, y2 in positions[i+1]:\n",
    "                svg += f'<line x1=\"{x1}\" y1=\"{y1}\" x2=\"{x2}\" y2=\"{y2}\" stroke=\"gray\" stroke-width=\"1\" />'\n",
    "    svg += '</svg>'\n",
    "    html = f'''\n",
    "    <style>\n",
    "        .network-wrapper {{\n",
    "            position: relative;\n",
    "            width: {total_width}px;\n",
    "            height: {total_height}px;\n",
    "        }}\n",
    "        .network {{\n",
    "            position: absolute;\n",
    "            top: 0; left: 0;\n",
    "            display: flex;\n",
    "            flex-direction: row;\n",
    "            justify-content: center;\n",
    "            height: 100%;\n",
    "        }}\n",
    "        .layer {{\n",
    "            display: flex;\n",
    "            flex-direction: column;\n",
    "            justify-content: center;\n",
    "            align-items: center;\n",
    "            width: {column_spacing}px;\n",
    "        }}\n",
    "        .neuron {{\n",
    "            width: {neuron_size}px;\n",
    "            height: {neuron_size}px;\n",
    "            border: 2px solid black;\n",
    "            border-radius: 50%;\n",
    "            background-color: #f2f2f2;\n",
    "            margin: 10px 0;\n",
    "            box-sizing: border-box;\n",
    "        }}\n",
    "    </style>\n",
    "    <div class=\"network-wrapper\">\n",
    "        {svg}\n",
    "        <div class=\"network\">\n",
    "    '''\n",
    "    for n_neurons in all_layers:\n",
    "        html += '<div class=\"layer\">'\n",
    "        for _ in range(n_neurons):\n",
    "            html += '<div class=\"neuron\"></div>'\n",
    "        html += '</div>'\n",
    "    html += '</div></div>'\n",
    "    display(HTML(html))\n",
    "\n",
    "# --- Speicher ---\n",
    "trained_model = {}\n",
    "\n",
    "# --- Training ---\n",
    "def train_model(X, y, layer_sizes, activations, epochs, lr):\n",
    "    dims = [len(X[0])] + layer_sizes + [1]\n",
    "    weights, biases = [], []\n",
    "    for i in range(len(dims)-1):\n",
    "        w = [[random.uniform(-0.5, 0.5) for _ in range(dims[i])] for _ in range(dims[i+1])]\n",
    "        b = [random.uniform(-0.5, 0.5) for _ in range(dims[i+1])]\n",
    "        weights.append(w)\n",
    "        biases.append(b)\n",
    "    loss_trace = []\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        for xi, yi in zip(X, y):\n",
    "            x = xi\n",
    "            acts, pres = [], []\n",
    "            for i in range(len(weights)):\n",
    "                act = 'sigmoid' if i == len(weights)-1 else activations[i]\n",
    "                x, pre = dense_forward(x, weights[i], biases[i], act)\n",
    "                acts.append(x)\n",
    "                pres.append(pre)\n",
    "            loss = binary_cross_entropy(acts[-1][0], yi)\n",
    "            total_loss += loss\n",
    "            grad = [binary_cross_entropy_derivative(acts[-1][0], yi)]\n",
    "            for i in reversed(range(len(weights))):\n",
    "                act = 'sigmoid' if i == len(weights)-1 else activations[i]\n",
    "                inp = xi if i == 0 else acts[i-1]\n",
    "                grad = dense_backward(inp, grad, acts[i], pres[i], weights[i], biases[i], act, lr)\n",
    "        loss_trace.append(total_loss)\n",
    "    return {\n",
    "        \"weights\": weights,\n",
    "        \"biases\": biases,\n",
    "        \"loss_trace\": loss_trace\n",
    "    }\n",
    "\n",
    "# --- Widgets ---\n",
    "layers_slider = widgets.IntSlider(value=2, min=1, max=5, step=1, description='Schichten:')\n",
    "neuron_and_activation_controls = widgets.VBox()\n",
    "epochs_slider = widgets.IntSlider(value=100, min=10, max=500, step=10, description='Epochen:')\n",
    "lr_slider = widgets.FloatSlider(value=0.05, min=0.001, max=1.0, step=0.01, description='Lernrate:')\n",
    "train_button = widgets.Button(description=\"Trainieren\")\n",
    "train_output = widgets.Output()\n",
    "\n",
    "def update_neuron_sliders(*args):\n",
    "    count = layers_slider.value\n",
    "    controls = []\n",
    "    for i in range(count):\n",
    "        neuron_slider = widgets.IntSlider(value=3, min=1, max=10, step=1, description=f'{i+1}. Schicht:')\n",
    "        activation_dropdown = widgets.Dropdown(\n",
    "            options=['sigmoid', 'relu', 'leaky_relu', 'tanh'],\n",
    "            value='sigmoid',\n",
    "            description='mit Funktion:'\n",
    "        )\n",
    "        controls.append(widgets.HBox([neuron_slider, activation_dropdown]))\n",
    "    neuron_and_activation_controls.children = controls\n",
    "\n",
    "layers_slider.observe(update_neuron_sliders, names='value')\n",
    "update_neuron_sliders()\n",
    "\n",
    "def on_train_click(b):\n",
    "    train_output.clear_output()\n",
    "    layer_sizes = []\n",
    "    activations = []\n",
    "    for control in neuron_and_activation_controls.children:\n",
    "        neuron_slider, activation_dropdown = control.children\n",
    "        layer_sizes.append(neuron_slider.value)\n",
    "        activations.append(activation_dropdown.value)\n",
    "    epochs = epochs_slider.value\n",
    "    lr = lr_slider.value\n",
    "    model = train_model(X, y, layer_sizes, activations, epochs, lr)\n",
    "    trained_model.clear()\n",
    "    trained_model.update({\n",
    "        \"weights\": model[\"weights\"],\n",
    "        \"biases\": model[\"biases\"],\n",
    "        \"loss_trace\": model[\"loss_trace\"],\n",
    "        \"layer_sizes\": layer_sizes\n",
    "    })\n",
    "    with train_output:\n",
    "        html_network_with_connections(layer_sizes, input_dim=len(X[0]))\n",
    "\n",
    "train_button.on_click(on_train_click)\n",
    "\n",
    "display(widgets.VBox([\n",
    "    widgets.HTML(\"<b>1. Anzahl an Schichten (N)</b>\"),\n",
    "    layers_slider,\n",
    "    widgets.HTML(\"<b>2. Anzahl an Neuronen (N) sowie Aktivierungsfunktionen</b>\"),\n",
    "    neuron_and_activation_controls,\n",
    "    widgets.HTML(\"<b>3. Anzahl an Epochen (N)</b>\"),\n",
    "    epochs_slider,\n",
    "    widgets.HTML(\"<b>4. H√∂he der Lernrate (%)</b>\"),\n",
    "    lr_slider,\n",
    "    train_button,\n",
    "    train_output\n",
    "]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2be5ede6-7df2-4aa5-989b-10f08950a6d8",
   "metadata": {},
   "source": [
    "## üìä - Evaluation\n",
    "Zur abschlie√üenden Evaluation des neuronalen Netzwerks wird der Verlauf der **Verlustfunktion** ([Wiki](https://en.wikipedia.org/wiki/Loss_functions_for_classification)) in den Epochen sowie eine **Confusion Matrix** ([Wiki](https://en.wikipedia.org/wiki/Confusion_matrix)) mit der dazugeh√∂rigen **Genauigkeit** ([Wiki](https://en.wikipedia.org/wiki/Accuracy_and_precision#In_classification)) in Prozent ausgewiesen. Sollten **False Positives** ([Wiki](https://en.wikipedia.org/wiki/False_positives_and_false_negatives#False_positive_error)) oder **False Negatives** ([Wiki](https://en.wikipedia.org/wiki/False_positives_and_false_negatives#False_negative_error)) vorliegen, so werden die entsprechenden **Reihen** ([Wiki](https://en.wikipedia.org/wiki/Row_(database))) f√ºr eine manuelle Sichtung zus√§tzlich ausgegeben. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cb7edc2-c13d-4fcc-a286-3e253a9e68d9",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# --- Dritter Teil des Codes ---\n",
    "eval_button = widgets.Button(description=\"Bewerten\")\n",
    "eval_output = widgets.Output()\n",
    "\n",
    "def predict(x, weights, biases):\n",
    "    for i in range(len(weights)):\n",
    "        act = 'sigmoid' if i == len(weights)-1 else 'relu'\n",
    "        x, _ = dense_forward(x, weights[i], biases[i], act)\n",
    "    return 1 if x[0] > 0.5 else 0\n",
    "\n",
    "def on_eval_click(b):\n",
    "    eval_output.clear_output()\n",
    "    if not trained_model:\n",
    "        with eval_output:\n",
    "            print(\"Bitte zuerst das Modell trainieren.\")\n",
    "        return\n",
    "\n",
    "    weights = trained_model[\"weights\"]\n",
    "    biases = trained_model[\"biases\"]\n",
    "    losses = trained_model[\"loss_trace\"]\n",
    "\n",
    "    ypred = []\n",
    "    TP = TN = FP = FN = 0\n",
    "    FP_indices = []\n",
    "    FN_indices = []\n",
    "\n",
    "    for idx, (true, xi) in enumerate(zip(y, X)):\n",
    "        pred = predict(xi, weights, biases)\n",
    "        ypred.append(pred)\n",
    "        if true == pred:\n",
    "            if true == 1: TP += 1\n",
    "            else: TN += 1\n",
    "        else:\n",
    "            if true == 1:\n",
    "                FN += 1\n",
    "                FN_indices.append(idx)\n",
    "            else:\n",
    "                FP += 1\n",
    "                FP_indices.append(idx)\n",
    "\n",
    "    acc = (TP + TN) / len(y)\n",
    "\n",
    "    with eval_output:\n",
    "        print(\"Verlust (jede 10. Epoche):\")\n",
    "        for i in range(9, len(losses), 10):\n",
    "            print(f\"Epoche {i+1:>3}: Verlust = {losses[i]:.4f}\")\n",
    "        print(\"\\nConfusion Matrix:\")\n",
    "        print(f\"TN: {TN}  FP: {FP}\")\n",
    "        print(f\"FN: {FN}  TP: {TP}\")\n",
    "        print(f\"Genauigkeit: {acc:.2f}\")\n",
    "        print()\n",
    "        print(\"\\n‚ùå False Positives bei Index:\", ', '.join(map(str, FP_indices)) if FP_indices else \"Keine\")\n",
    "        print(\"‚ùå False Negatives bei Index:\", ', '.join(map(str, FN_indices)) if FN_indices else \"Keine\")\n",
    "\n",
    "eval_button.on_click(on_eval_click)\n",
    "display(widgets.VBox([eval_button, eval_output]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bd747fd-636a-4638-8513-d97002e61f9d",
   "metadata": {},
   "source": [
    "![Logo](https://github.com/statistical-thinking/ki-enna-demo/blob/main/content/img/statistical-thinking.png?raw=true)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  },
  "toc": {
   "base_numbering": 0
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
