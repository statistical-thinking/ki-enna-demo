{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a005a762-a9c7-43c5-9e96-c1520595328d",
   "metadata": {},
   "source": [
    "# KI-ENNA: (E)in (N)euronales (N)etz zum (A)usprobieren\n",
    "Live-Demo basierend auf KI-ENNA 2.0.1 von Prof. Dr. habil. Dennis Klinkhammer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c083f5f-f8f7-413d-b0f4-d06fc726f49c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "![title](https://github.com/statistical-thinking/ki-enna-demo/blob/main/content/img/image1.png?raw=true)\n",
    "## Aufgabenstellung\n",
    "Die Darstellung zeigt drei **verschiedene Pflanzenarten**, deren **Blattlänge und Blattbreite** gemessen worden sind. Während die mit Kreisen dargestellte Planzenart sich eindeutig von den beiden anderen Pflanzenarten unterscheidet, weisen von den anderen Pflanzenarten **einzelne Pflanzen die gleiche Blattlänge und Blattbreite** auf. Daher sind diese Pflanzenarten **schwer zu unterscheiden**. Ein neuronales Netzwerk kann bei der Unterscheidung der ähnlichen Pflanzenarten hilfreich sein. Finde zunächst heraus, **ob ein großes neuronales Netzwerk erforderlich ist**, oder ob ein kleines neuronales Netzwerk die Unterscheidung ebenso gut erledigen kann. In einem weiteren Schritt kannst Du **mit den zugrundliegenden Aktivierungsfunktionen sowie den Epochen und der Lernrate experimentieren**, um die Auswirkungen des Lernverhaltens auf das neuronale Netzwerk beobachten zu können."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9358d05a-3cad-42a7-8617-1ffc99df1a7f",
   "metadata": {},
   "source": [
    "## Training des neuronalen Netzwerks\n",
    "Hier kannst Du die Anzahl an Schichten, Neuronen (und deren Aktivierungsfunktionen) sowie Epochen, aber auch die Lernrate selbst einstellen. Neuronale Netzwerke sind wie kleine Denkmaschinen, **die in Schichten arbeitet**. In die **Eingabeschicht** fließen Zahlen aus Datensätzen ein, die in den **versteckten Schichten** weiter verarbeitet werden. Die **Ausgabeschicht** liefert das Ergebnis. **Neuronen treiben die Denkmaschine an**. Sie rechnen auf der Grundlage der festgelegten **Aktivierungsfunktionen** mit den Zahlen und **schicken Zwischenergebnisse an Neuronen in anderen Schichten weiter**. Aber Achtung: Nicht immer führen mehr Schichten und Neuronen zu einem besseren Ergebnis. Auch falsch ausgewählte Aktivierungsfunktionen können das Ergebnis verfälschen. Damit das funktioniert, braucht ein neuronales Netzwerk Wiederholungen: Eine **Epoche ist ein Durchgang durch alle Übungsaufgaben**. Das macht das neuronale Netzwerk mehrmals, um immer besser zu werden. Die **Lernrate bestimmt, wie schnell das neuronale Netzwerk dabei seine Fehler verbessert**: Ist die Lernrate zu hoch, lernt es zu schnell und macht vielleicht neue Fehler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "90cd7138-8ae0-4dff-9675-306e0e8e6333",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a5ad46072534ff281280296da6c07a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<b>1. Anzahl an Schichten (N)</b>'), IntSlider(value=2, description='Schichten:', m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# --- Erster Teil des Codes ---\n",
    "import micropip\n",
    "await micropip.install('ipywidgets')\n",
    "import random, math\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, clear_output, HTML\n",
    "\n",
    "# --- Dataset ---\n",
    "Xtest = [[ 0.81575475, -0.21746808, -0.12904165, -0.65303909],\n",
    "         [ 0.05761837,  1.59476592,  0.84485761,  1.71304456],\n",
    "         [ 0.96738203,  0.68864892, -0.00730424, -0.41643072],\n",
    "         [ 2.02877297,  0.38660992,  2.06223168,  1.00321947],\n",
    "         [ 1.42226386,  0.99068792,  1.33180724,  0.29339437],\n",
    "         [ 0.81575475,  0.99068792,  1.21006983,  1.4764362 ],\n",
    "         [-1.00377258,  0.38660992, -0.49425387, -0.41643072],\n",
    "         [ 0.05761837, -0.51950708, -0.00730424,  0.29339437],\n",
    "         [ 0.36087292,  0.38660992,  1.08833242,  1.23982783],\n",
    "         [ 0.66412748,  0.38660992,  0.35790798,  1.4764362 ],\n",
    "         [ 0.05761837,  0.08457092,  0.84485761,  0.29339437],\n",
    "         [-0.70051802, -0.51950708,  0.23617057,  0.53000274],\n",
    "         [ 0.20924564, -0.21746808,  0.84485761,  1.00321947],\n",
    "         [-0.24563619,  0.08457092, -0.25077906, -0.65303909],\n",
    "         [-2.06516352, -1.42562408, -1.95510276, -1.59947255],\n",
    "         [-1.15539985, -1.42562408, -1.34641572, -1.36286418],\n",
    "         [ 0.05761837, -1.12358508, -0.00730424, -0.41643072],\n",
    "         [ 0.20924564,  0.08457092, -0.73772869, -0.88964745],\n",
    "         [-0.39726347, -0.51950708,  0.23617057, -0.17982236],\n",
    "         [ 0.5125002 ,  0.08457092, -0.37251647, -0.88964745]]\n",
    "ytrue = [0,1,0,1,1,1,0,1,1,1,1,1,1,0,0,0,0,0,0,0]\n",
    "\n",
    "# --- Activation & Loss ---\n",
    "def sigmoid(x): return 1 / (1 + math.exp(-x))\n",
    "def sigmoid_derivative(out): return out * (1 - out)\n",
    "def relu(x): return max(0, x)\n",
    "def relu_derivative(out): return 1 if out > 0 else 0\n",
    "def leaky_relu(x): return x if x > 0 else 0.01 * x\n",
    "def leaky_relu_derivative(out): return 1 if out > 0 else 0.01\n",
    "def tanh(x): return math.tanh(x)\n",
    "def tanh_derivative(out): return 1 - out**2\n",
    "def binary_cross_entropy(pred, y): \n",
    "    epsilon = 1e-7\n",
    "    return - (y * math.log(pred + epsilon) + (1 - y) * math.log(1 - pred + epsilon))\n",
    "def binary_cross_entropy_derivative(pred, y): \n",
    "    epsilon = 1e-7\n",
    "    return -(y / (pred + epsilon)) + (1 - y) / (1 - pred + epsilon)\n",
    "\n",
    "# --- Dense Layer ---\n",
    "def dense_forward(x, w, b, act='relu'):\n",
    "    pres = [sum(x[i] * w[j][i] for i in range(len(x))) + b[j] for j in range(len(w))]\n",
    "    outs = []\n",
    "    for z in pres:\n",
    "        if act == 'sigmoid':\n",
    "            outs.append(sigmoid(z))\n",
    "        elif act == 'relu':\n",
    "            outs.append(relu(z))\n",
    "        elif act == 'leaky_relu':\n",
    "            outs.append(leaky_relu(z))\n",
    "        elif act == 'tanh':\n",
    "            outs.append(tanh(z))\n",
    "    return outs, pres\n",
    "\n",
    "def dense_backward(x, grad_out, out, pre, w, b, act='relu', lr=0.01):\n",
    "    grad_in = [0] * len(x)\n",
    "    for j in range(len(w)):\n",
    "        if act == 'sigmoid':\n",
    "            delta = grad_out[j] * sigmoid_derivative(out[j])\n",
    "        elif act == 'relu':\n",
    "            delta = grad_out[j] * relu_derivative(pre[j])\n",
    "        elif act == 'leaky_relu':\n",
    "            delta = grad_out[j] * leaky_relu_derivative(pre[j])\n",
    "        elif act == 'tanh':\n",
    "            delta = grad_out[j] * tanh_derivative(out[j])\n",
    "        for i in range(len(x)):\n",
    "            grad_in[i] += w[j][i] * delta\n",
    "            w[j][i] -= lr * delta * x[i]\n",
    "        b[j] -= lr * delta\n",
    "    return grad_in\n",
    "\n",
    "# --- Visualisierung ---\n",
    "def html_network_with_connections(layer_sizes, input_dim):\n",
    "    all_layers = [input_dim] + layer_sizes + [1]\n",
    "    neuron_size = 20\n",
    "    margin = 10\n",
    "    column_spacing = 80\n",
    "    row_spacing = neuron_size + 20\n",
    "    radius = neuron_size // 2\n",
    "    total_width = len(all_layers) * column_spacing\n",
    "    total_height = max(all_layers) * row_spacing + 2 * margin\n",
    "    svg = f'<svg width=\"{total_width}\" height=\"{total_height}\" style=\"position:absolute; top:0; left:0;\">'\n",
    "    positions = []\n",
    "    for li, n in enumerate(all_layers):\n",
    "        layer_x = li * column_spacing + column_spacing // 2\n",
    "        layer = []\n",
    "        total_layer_height = (n - 1) * row_spacing\n",
    "        offset_y = (total_height - total_layer_height) // 2\n",
    "        for ni in range(n):\n",
    "            layer_y = offset_y + ni * row_spacing\n",
    "            layer.append((layer_x, layer_y))\n",
    "        positions.append(layer)\n",
    "    for i in range(len(positions)-1):\n",
    "        for x1, y1 in positions[i]:\n",
    "            for x2, y2 in positions[i+1]:\n",
    "                svg += f'<line x1=\"{x1}\" y1=\"{y1}\" x2=\"{x2}\" y2=\"{y2}\" stroke=\"gray\" stroke-width=\"1\" />'\n",
    "    svg += '</svg>'\n",
    "    html = f'''\n",
    "    <style>\n",
    "        .network-wrapper {{\n",
    "            position: relative;\n",
    "            width: {total_width}px;\n",
    "            height: {total_height}px;\n",
    "        }}\n",
    "        .network {{\n",
    "            position: absolute;\n",
    "            top: 0; left: 0;\n",
    "            display: flex;\n",
    "            flex-direction: row;\n",
    "            justify-content: center;\n",
    "            height: 100%;\n",
    "        }}\n",
    "        .layer {{\n",
    "            display: flex;\n",
    "            flex-direction: column;\n",
    "            justify-content: center;\n",
    "            align-items: center;\n",
    "            width: {column_spacing}px;\n",
    "        }}\n",
    "        .neuron {{\n",
    "            width: {neuron_size}px;\n",
    "            height: {neuron_size}px;\n",
    "            border: 2px solid black;\n",
    "            border-radius: 50%;\n",
    "            background-color: #f2f2f2;\n",
    "            margin: 10px 0;\n",
    "            box-sizing: border-box;\n",
    "        }}\n",
    "    </style>\n",
    "    <div class=\"network-wrapper\">\n",
    "        {svg}\n",
    "        <div class=\"network\">\n",
    "    '''\n",
    "    for n_neurons in all_layers:\n",
    "        html += '<div class=\"layer\">'\n",
    "        for _ in range(n_neurons):\n",
    "            html += '<div class=\"neuron\"></div>'\n",
    "        html += '</div>'\n",
    "    html += '</div></div>'\n",
    "    display(HTML(html))\n",
    "\n",
    "# --- Speicher ---\n",
    "trained_model = {}\n",
    "\n",
    "# --- Training ---\n",
    "def train_model(X, y, layer_sizes, activations, epochs, lr):\n",
    "    dims = [len(X[0])] + layer_sizes + [1]\n",
    "    weights, biases = [], []\n",
    "    for i in range(len(dims)-1):\n",
    "        w = [[random.uniform(-0.5, 0.5) for _ in range(dims[i])] for _ in range(dims[i+1])]\n",
    "        b = [random.uniform(-0.5, 0.5) for _ in range(dims[i+1])]\n",
    "        weights.append(w)\n",
    "        biases.append(b)\n",
    "    loss_trace = []\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        for xi, yi in zip(X, y):\n",
    "            x = xi\n",
    "            acts, pres = [], []\n",
    "            for i in range(len(weights)):\n",
    "                act = 'sigmoid' if i == len(weights)-1 else activations[i]\n",
    "                x, pre = dense_forward(x, weights[i], biases[i], act)\n",
    "                acts.append(x)\n",
    "                pres.append(pre)\n",
    "            loss = binary_cross_entropy(acts[-1][0], yi)\n",
    "            total_loss += loss\n",
    "            grad = [binary_cross_entropy_derivative(acts[-1][0], yi)]\n",
    "            for i in reversed(range(len(weights))):\n",
    "                act = 'sigmoid' if i == len(weights)-1 else activations[i]\n",
    "                inp = xi if i == 0 else acts[i-1]\n",
    "                grad = dense_backward(inp, grad, acts[i], pres[i], weights[i], biases[i], act, lr)\n",
    "        loss_trace.append(total_loss)\n",
    "    return {\n",
    "        \"weights\": weights,\n",
    "        \"biases\": biases,\n",
    "        \"loss_trace\": loss_trace\n",
    "    }\n",
    "\n",
    "# --- Widgets ---\n",
    "layers_slider = widgets.IntSlider(value=2, min=1, max=5, step=1, description='Schichten:')\n",
    "neuron_and_activation_controls = widgets.VBox()\n",
    "epochs_slider = widgets.IntSlider(value=100, min=10, max=500, step=10, description='Epochen:')\n",
    "lr_slider = widgets.FloatSlider(value=0.05, min=0.001, max=1.0, step=0.01, description='Lernrate:')\n",
    "train_button = widgets.Button(description=\"Trainieren\")\n",
    "train_output = widgets.Output()\n",
    "\n",
    "def update_neuron_sliders(*args):\n",
    "    count = layers_slider.value\n",
    "    controls = []\n",
    "    for i in range(count):\n",
    "        neuron_slider = widgets.IntSlider(value=4, min=1, max=10, step=1, description=f'{i+1}. Schicht:')\n",
    "        activation_dropdown = widgets.Dropdown(\n",
    "            options=['relu', 'leaky_relu', 'sigmoid', 'tanh'],\n",
    "            value='relu',\n",
    "            description='mit Funktion:'\n",
    "        )\n",
    "        controls.append(widgets.HBox([neuron_slider, activation_dropdown]))\n",
    "    neuron_and_activation_controls.children = controls\n",
    "\n",
    "layers_slider.observe(update_neuron_sliders, names='value')\n",
    "update_neuron_sliders()\n",
    "\n",
    "def on_train_click(b):\n",
    "    train_output.clear_output()\n",
    "    layer_sizes = []\n",
    "    activations = []\n",
    "    for control in neuron_and_activation_controls.children:\n",
    "        neuron_slider, activation_dropdown = control.children\n",
    "        layer_sizes.append(neuron_slider.value)\n",
    "        activations.append(activation_dropdown.value)\n",
    "    epochs = epochs_slider.value\n",
    "    lr = lr_slider.value\n",
    "    model = train_model(Xtest, ytrue, layer_sizes, activations, epochs, lr)\n",
    "    trained_model.clear()\n",
    "    trained_model.update({\n",
    "        \"weights\": model[\"weights\"],\n",
    "        \"biases\": model[\"biases\"],\n",
    "        \"loss_trace\": model[\"loss_trace\"],\n",
    "        \"layer_sizes\": layer_sizes\n",
    "    })\n",
    "    with train_output:\n",
    "        html_network_with_connections(layer_sizes, input_dim=len(Xtest[0]))\n",
    "\n",
    "train_button.on_click(on_train_click)\n",
    "\n",
    "display(widgets.VBox([\n",
    "    widgets.HTML(\"<b>1. Anzahl an Schichten (N)</b>\"),\n",
    "    layers_slider,\n",
    "    widgets.HTML(\"<b>2. Anzahl an Neuronen (N) sowie Aktivierungsfunktionen</b>\"),\n",
    "    neuron_and_activation_controls,\n",
    "    widgets.HTML(\"<b>3. Anzahl an Epochen (N)</b>\"),\n",
    "    epochs_slider,\n",
    "    widgets.HTML(\"<b>4. Höhe der Lernrate (%)</b>\"),\n",
    "    lr_slider,\n",
    "    train_button,\n",
    "    train_output\n",
    "]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15a39894-fbf6-4a4c-9e4d-2f719efc2ffe",
   "metadata": {},
   "source": [
    "## Bewertung de neuronalen Netzwerks\n",
    "Die **Verlust-Funktion ist wie ein Schiedsrichter**. Sie schaut sich die Antworten des neuronalen Netzwerks an und entscheidet, ob das Ergebnis richtig oder falsch ist. Wenn ein neuronales Netzwerk falsch liegt, zeigt die Verlust-Funktion **wie weit die Antwort von der richtigen Lösung entfernt ist**. So lernt das neuronale Netzwerk, was es besser machen muss. Die Antworten eines neuronalen Netzwerks können in einer **Confusion Matrix** zusammengefasst werde. Das ist eine **Tabelle, die zeigt, wie oft ein neuronales Netzwerk etwas richtig oder falsch erkannt hat**. Daraus kann man dann die sogenannte **Accuracy** berechnen, also **wie genau das neuronale Netzwerk insgesamt war**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5cb7edc2-c13d-4fcc-a286-3e253a9e68d9",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "19970a426508470ea13d6eb429791317",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Button(description='Bewerten', style=ButtonStyle()), Output()))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# --- Zweiter Teil des Codes ---\n",
    "from IPython.display import display\n",
    "eval_button = widgets.Button(description=\"Bewerten\")\n",
    "eval_output = widgets.Output()\n",
    "\n",
    "def predict(x, weights, biases):\n",
    "    for i in range(len(weights)):\n",
    "        act = 'sigmoid' if i == len(weights)-1 else 'relu'\n",
    "        x, _ = dense_forward(x, weights[i], biases[i], act)\n",
    "    return 1 if x[0] > 0.5 else 0\n",
    "\n",
    "def on_eval_click(b):\n",
    "    eval_output.clear_output()\n",
    "    if not trained_model:\n",
    "        with eval_output:\n",
    "            print(\"Bitte zuerst das Modell trainieren.\")\n",
    "        return\n",
    "\n",
    "    weights = trained_model[\"weights\"]\n",
    "    biases = trained_model[\"biases\"]\n",
    "    losses = trained_model[\"loss_trace\"]\n",
    "\n",
    "    ypred = [predict(xi, weights, biases) for xi in Xtest]\n",
    "    TP = TN = FP = FN = 0\n",
    "    for true, pred in zip(ytrue, ypred):\n",
    "        if true == pred:\n",
    "            if true == 1: TP += 1\n",
    "            else: TN += 1\n",
    "        else:\n",
    "            if true == 1: FN += 1\n",
    "            else: FP += 1\n",
    "    acc = (TP + TN) / len(ytrue)\n",
    "\n",
    "    with eval_output:\n",
    "        print(\"Verlust (jede 10. Epoche):\")\n",
    "        for i in range(9, len(losses), 10):\n",
    "            print(f\"Epoche {i+1:>3}: Verlust = {losses[i]:.4f}\")\n",
    "        print(\"\\nConfusion Matrix:\")\n",
    "        print(f\"TN: {TN}  FP: {FP}\")\n",
    "        print(f\"FN: {FN}  TP: {TP}\")\n",
    "        print(f\"Genauigkeit: {acc:.2f}\")\n",
    "\n",
    "eval_button.on_click(on_eval_click)\n",
    "display(widgets.VBox([eval_button, eval_output]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dadf91b1-8928-422b-a25d-f7385cc3e6ab",
   "metadata": {},
   "source": [
    "**Prima**, jetzt hast Du ein eigenes neuronales Netzwerk **trainiert und bewertet**!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "230f72dd-8506-472d-934e-c4a7341c38a0",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  },
  "toc": {
   "base_numbering": 0
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
